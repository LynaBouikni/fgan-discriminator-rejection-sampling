# F-GAN with Discriminator Rejection Sampling

> Training GANs on MNIST Â· Variational Divergence Minimization Â· Rejection Sampling Â· Precision-Recall Trade-off

---

## ğŸ“Œ Overview

This project investigates enhancements to Vanilla Generative Adversarial Networks (GANs) using two core methods:

- **F-GAN**: Extends GANs by optimizing f-divergences (e.g. KLD, JS, BCE) instead of relying only on Jensen-Shannon divergence.
- **Discriminator Rejection Sampling (DRS)**: A sampling method that leverages discriminator outputs to accept high-quality synthetic data.

All experiments are conducted on the **MNIST dataset** and evaluated with **FID**, **precision**, and **recall**.

---

## ğŸ¯ Objectives

- âœ… Train a baseline Vanilla GAN with fixed architecture.
- âœ… Compare different f-divergence losses (BCE, JS, KLD).
- âœ… Implement DRS to refine sample quality and diversity.
- âœ… Analyze visual and quantitative performance over multiple epochs.

---

## ğŸ—‚ Dataset

- **MNIST**: 28x28 grayscale images of handwritten digits.
- **Training Set**: 60,000 samples  
- **Test Set**: 10,000 samples  
- Preprocessed into flat 784-dim vectors.

---

## ğŸ§  Model Architectures

### ğŸ›ï¸ Vanilla GAN

- **Generator**: Fully connected feed-forward MLP with LeakyReLU activations and final Tanh.
- **Discriminator**: MLP with sigmoid output for binary classification.

```text
Generator: z âˆˆ â„Â¹â°â° â†’ G(z) âˆˆ â„â·â¸â´
Discriminator: x âˆˆ â„â·â¸â´ â†’ D(x) âˆˆ [0,1]
Trained for 50 epochs with batch size = 64 and lr = 0.0001
```
ğŸ”€ f-GAN
Instead of minimizing the original GAN loss, we minimize f-divergences:

BCE: Binary Cross Entropy (baseline)

KLD: Kullback-Leibler Divergence

JS: Jensen-Shannon Divergence

Each divergence affects convergence behavior, quality, and diversity.

ğŸ” Discriminator Rejection Sampling (DRS)
DRS uses the discriminatorâ€™s confidence scores to accept or reject generated samples:

Discriminator Scoring: Scores all generated images.

Acceptance Probability: Uses sigmoid-adjusted function based on max score M and a hyperparameter Î³.

Dynamic Thresholding: M is updated online.

Sample Selection: Repeats until 10,000 accepted samples are saved.

ğŸ“Š Results
Model	Time (s)	FID	Precision	Recall
Vanilla GAN	111.4	52.44	0.52	0.18
F-GAN (JS, 100 epochs)	~	~	â†‘	â†‘
F-GAN + DRS (KLD, 50e)	~	â†“	â†‘â†‘	â†‘â†‘

ğŸ” DRS significantly improved both precision and diversity compared to plain GANs.

ğŸ“ Code Structure
```bash
â”œâ”€â”€ fgan.py                  # F-GAN training logic
â”œâ”€â”€ drs.py                   # Discriminator Rejection Sampling implementation
â”œâ”€â”€ utils.py                 # Evaluation, divergence functions, helpers
â”œâ”€â”€ train_vanilla.py         # Vanilla GAN baseline training
â”œâ”€â”€ plots/                   # Generated image outputs
â”œâ”€â”€ samples/                 # DRS-accepted images
â”œâ”€â”€ README.md                # You're here!
```
