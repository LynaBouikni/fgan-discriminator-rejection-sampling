# F-GAN with Discriminator Rejection Sampling

> Training GANs on MNIST Â· Variational Divergence Minimization Â· Rejection Sampling Â· Precision-Recall Trade-off

---

## ğŸ“Œ Overview

This project investigates enhancements to Vanilla Generative Adversarial Networks (GANs) using two core methods:

- **F-GAN**: Extends GANs by optimizing f-divergences (e.g. KLD, JS, BCE) instead of relying only on Jensen-Shannon divergence.
- **Discriminator Rejection Sampling (DRS)**: A sampling method that leverages discriminator outputs to accept high-quality synthetic data.

All experiments are conducted on the **MNIST dataset** and evaluated with **FID**, **precision**, and **recall**.

---

## ğŸ¯ Objectives

- âœ… Train a baseline Vanilla GAN with fixed architecture.
- âœ… Compare different f-divergence losses (BCE, JS, KLD).
- âœ… Implement DRS to refine sample quality and diversity.
- âœ… Analyze visual and quantitative performance over multiple epochs.

---

## ğŸ—‚ Dataset

- **MNIST**: 28x28 grayscale images of handwritten digits.
- **Training Set**: 60,000 samples  
- **Test Set**: 10,000 samples  
- Preprocessed into flat 784-dim vectors.

---

## ğŸ§  Model Architectures

### ğŸ›ï¸ Vanilla GAN

- **Generator**: Fully connected feed-forward MLP with LeakyReLU activations and final Tanh.
- **Discriminator**: MLP with sigmoid output for binary classification.

```text
Generator: z âˆˆ â„Â¹â°â° â†’ G(z) âˆˆ â„â·â¸â´
Discriminator: x âˆˆ â„â·â¸â´ â†’ D(x) âˆˆ [0,1]
Trained for 50 epochs with batch size = 64 and lr = 0.0001
```
ğŸ”€ f-GAN
Instead of minimizing the original GAN loss, we minimize f-divergences:

BCE: Binary Cross Entropy (baseline)

KLD: Kullback-Leibler Divergence

JS: Jensen-Shannon Divergence

Each divergence affects convergence behavior, quality, and diversity.

ğŸ” Discriminator Rejection Sampling (DRS)
DRS uses the discriminatorâ€™s confidence scores to accept or reject generated samples:

Discriminator Scoring: Scores all generated images.

Acceptance Probability: Uses sigmoid-adjusted function based on max score M and a hyperparameter Î³.

Dynamic Thresholding: M is updated online.

Sample Selection: Repeats until 10,000 accepted samples are saved.

ğŸ“Š Results
Model	Time (s)	FID	Precision	Recall
Vanilla GAN	111.4	52.44	0.52	0.18
F-GAN (JS, 100 epochs)	~	~	â†‘	â†‘
F-GAN + DRS (KLD, 50e)	~	â†“	â†‘â†‘	â†‘â†‘

ğŸ” DRS significantly improved both precision and diversity compared to plain GANs.

ğŸ“ Code Structure
```bash
â”œâ”€â”€ fgan.py                  # F-GAN training logic
â”œâ”€â”€ drs.py                   # Discriminator Rejection Sampling implementation
â”œâ”€â”€ utils.py                 # Evaluation, divergence functions, helpers
â”œâ”€â”€ train_vanilla.py         # Vanilla GAN baseline training
â”œâ”€â”€ plots/                   # Generated image outputs
â”œâ”€â”€ samples/                 # DRS-accepted images
â”œâ”€â”€ README.md                # You're here!
```
ğŸ§¾ Dependencies
```bash
torch==2.0.1
numpy==1.25.1
scikit-learn==1.3.0
scipy==1.11.1
matplotlib
```

Install with:
```bash
pip install -r requirements.txt
```
ğŸ§ª Insights
ğŸ” f-divergence choice affects convergence: JS diverges slower than BCE but provides more stable training.

ğŸ¯ DRS boosts recall and image diversity at low computational cost.

ğŸ§­ Optimal epochs vary by divergence: JS and KLD plateau early, while BCE improves slowly over time.

ğŸ“š References
Nowozin et al., f-GAN: Variational Divergence Minimization

Azadi et al., Discriminator Rejection Sampling

ğŸ‘¥ Authors
The GANtastics

Lyna Bouikni

Arij Boubaker

Ãngel Luque

ğŸ§‘â€ğŸ“ M2 IASD â€” UniversitÃ© Paris Dauphine, 2023

ğŸ“¬ Contact
ğŸ“« Email: lynabouiknia@gmail.com
ğŸ”— LinkedIn

â€œPrecision means nothing without diversity â€” and diversity is everything in generative modeling.â€
